teacher_model_path: "Qwen/Qwen2-1.5B-Instruct"
student_model_path: "Qwen/Qwen2-0.5B-Instruct"

teacher_quantization_bit: 4
student_quantization_bit: 4

wandb_project: qwen2-0.5b_distill_qlora

lora_target: all
lora_rank: 64
lora_alpha: 32
lora_dropout: 0.05

dataset: mlabonne/FineTome-100k
max_seq_length: 2048
num_samples: 1000
num_proc: 8

output_dir: adapters/qwen2-0.5b_distill_qlora
logging_steps: 1
save_steps: 500

per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 2.0e-5
num_train_epochs: 1
lr_scheduler_type: cosine
warmup_ratio: 0.01
fp16: true
bf16: false
flash_attn: eager
resume_from_checkpoint: False

val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500

distillation_temperature: 2.0
distillation_alpha: 0.5